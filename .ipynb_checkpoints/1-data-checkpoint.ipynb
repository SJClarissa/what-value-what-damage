{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from time import sleep\n",
    "import os\n",
    "from pathlib import Path\n",
    "from urllib.request import urlopen\n",
    "import sqlalchemy as sal\n",
    "import copy\n",
    "\n",
    "from uszipcode import SearchEngine\n",
    "search = SearchEngine(simple_zipcode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"quandl.key\", \"r\") as myfile:\n",
    "    quandl_key=myfile.read().strip('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_value(number): \n",
    "    return (\"{:,}\".format(number)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wkb_hexer(line):\n",
    "    return line.wkb_hex\n",
    "    # https://stackoverflow.com/a/38363154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indic_loop(indics):\n",
    "    for indic in indics:\n",
    "        q_code = 'Z' + zipcode + '_' + indic\n",
    "        ftype = '.json'\n",
    "        params = {\n",
    "                'api_key' : api_key,\n",
    "            }\n",
    "        get_url = base_url+q_code+ftype\n",
    "        res = requests.get(get_url,params)\n",
    "        zip_code_data[indic] = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quandl(zipcode):\n",
    "    indics = ['ZHVIAH', 'ZHVIBT', 'ZHVIMT', 'ZHVITT', \"MVALFAH\"]\n",
    "    base_url = 'https://www.quandl.com/api/v3/datasets/ZILLOW/'\n",
    "    api_key = quandl_key\n",
    "    zip_code_data = {}\n",
    "    for indic in indics:\n",
    "        q_code = 'Z' + zipcode + '_' + indic\n",
    "        ftype = '.json'\n",
    "        params = {\n",
    "                'api_key' : api_key,\n",
    "            }\n",
    "        get_url = base_url+q_code+ftype\n",
    "        res = requests.get(get_url,params)\n",
    "        zip_code_data[indic] = res.json()\n",
    "#         for key in zip_code_data[indic].keys():\n",
    "#             if key == 'quandl_error':\n",
    "#                 if zip_code_data[indic]['quandl_error'] == 'QELx02':\n",
    "#                     print()\n",
    "#             zip_code_data[indic] = res.json()\n",
    "        \n",
    "    return(zip_code_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hurricane_damage(zipcode, disaster_level):\n",
    "    try: \n",
    "        level = {\n",
    "            1: 119, 2: 154, 3: 178, 4:209, 5: 252\n",
    "        }\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    search = SearchEngine(simple_zipcode=False)\n",
    "    z_dict = search.by_zipcode(zipcode)\n",
    "    if z_dict.zipcode_type == 'PO Box':\n",
    "        population = 0\n",
    "    else:\n",
    "        population = z_dict.population\n",
    "    \n",
    "    damage = -976532 + 2.22*(level[disaster_level])**3 + 9.81e-10*(population)**3\n",
    "    damage = np.round(damage,2)\n",
    "    return(damage)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flood_loss(zip_code, water_inches, avg_sq_ft):\n",
    "    \n",
    "    # getting number of houses in zipcode. setting to n_houses\n",
    "    search = SearchEngine(simple_zipcode=False)\n",
    "    z_dict = search.by_zipcode(zip_code)\n",
    "    if z_dict.zipcode_type == 'PO Box':\n",
    "        n_houses = 0\n",
    "    else:\n",
    "        n_houses = z_dict.housing_units\n",
    "    \n",
    "    water_inches = round(water_inches)\n",
    "    \n",
    "    inches = [13,14,15,16,17,18,19,20,21,22,23,25,26,27,28,29,30,31,32,33,34,35,37,38,39,40,41,42,43,44,45,46,47]\n",
    "    \n",
    "    if water_inches in inches:\n",
    "        print(f'Not data for flood of {water_inches} inches')\n",
    "        return\n",
    "    elif water_inches > 48:\n",
    "        print(\"No data for flood over 48 inches\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    if avg_sq_ft < 1000:\n",
    "    \n",
    "        small_home = {1: 9550, 2: 9620, 3: 9820, 4: 12730, 5: 12780,\n",
    "                      6: 15300, 7: 15508, 8: 15717, 9: 15925, 10: 16133,\n",
    "                      11: 16342, 12: 16550, 24: 19500, 36: 21100, 48: 23400} \n",
    "        damage = small_home[water_inches] * n_houses\n",
    "    \n",
    "    \n",
    "    elif avg_sq_ft >= 1000 and avg_sq_ft < 5000:\n",
    "    \n",
    "        average_home = {1: 23635, 2: 23720, 3: 24370, 4: 31345, 5: 31425, \n",
    "                        6: 37260, 7: 37691, 8: 38122, 9:38553, 10: 38983,\n",
    "                        11: 39414, 12: 39845, 24: 44325, 36: 47905, 48: 53355}\n",
    "        damage = average_home[water_inches] * n_houses\n",
    "    \n",
    "    \n",
    "    elif avg_sq_ft > 5000:\n",
    "    \n",
    "        large_home = {1: 47110, 2: 47220, 3: 48620, 4: 62370, 5: 62500, 6: 73860,\n",
    "                      7: 74662 , 8: 75463, 9: 76265, 10: 77067, 11: 77868, 12: 78670,\n",
    "                      24: 85700, 36: 92580, 48: 103280}\n",
    "        damage = large_home[water_inches] * n_houses\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(\"Square footage not found\")\n",
    "   \n",
    "    \n",
    "#     print(f'{zip_code} occured ${place_value(damage)} in damage from {water_inches} inches of flooding inside homes.')\n",
    "    return(damage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An average tornado covers 1.081 square miles\n",
    "# Percentage in values of dictionary are percentage of home value lost\n",
    "# category of tornados: 0,1,2,3,4,5\n",
    "# function = amount of damage in dollars * number of houses * percent of area hit by tornado\n",
    "\n",
    "def tornado_damage(zipcode, category, low_zest, mid_zest, top_zest):\n",
    "    search = SearchEngine(simple_zipcode=False)\n",
    "    z_dict = search.by_zipcode(zipcode)\n",
    "    if z_dict.zipcode_type == 'PO Box':\n",
    "        sq_miles = 0\n",
    "        n_houses = 0\n",
    "\n",
    "        percent_hit = 0\n",
    "    else:\n",
    "        \n",
    "        sq_miles = z_dict.land_area_in_sqmi\n",
    "        n_houses = z_dict.housing_units\n",
    "\n",
    "        percent_hit = 1.081/sq_miles\n",
    "    \n",
    "#     low_zest = define low_zest here\n",
    "#     mid_zest = define mid_zest here\n",
    "#     top_zest = define top_zest here\n",
    "    \n",
    "    if category == 0:\n",
    "        \n",
    "        houses = {'low_zest': low_zest*.002,\n",
    "                 'mid_zest': mid_zest*.002,\n",
    "                 'top_zest': top_zest*.002}\n",
    "    \n",
    "    elif category == 1:\n",
    "        \n",
    "        houses = {'low_zest': low_zest*.008,\n",
    "                 'mid_zest': mid_zest*.008,\n",
    "                 'top_zest': top_zest*.008}\n",
    "        \n",
    "    elif category == 2:\n",
    "        \n",
    "        houses = {'low_zest': low_zest*.016,\n",
    "                 'mid_zest': mid_zest*.016,\n",
    "                 'top_zest': top_zest*.016}\n",
    "        \n",
    "    elif category == 3:\n",
    "    \n",
    "        houses = {'low_zest': low_zest*.3,\n",
    "                 'mid_zest': mid_zest*.3,\n",
    "                 'top_zest': top_zest*.3}\n",
    "    \n",
    "    elif category == 4:\n",
    "        \n",
    "        houses = {'low_zest': low_zest*.70,\n",
    "                 'mid_zest': mid_zest*.70,\n",
    "                 'top_zest': top_zest*.70}\n",
    "        \n",
    "    elif category == 5:\n",
    "    \n",
    "        houses = {'low_zest': low_zest*1,\n",
    "                 'mid_zest': mid_zest*1,\n",
    "                 'top_zest': top_zest*1}\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"Acceptable tornado categories: 0, 1, 2, 3, 4, 5.\")\n",
    "    \n",
    "    \n",
    "    # damage function for each house tier\n",
    "    damage_low_tier = houses['low_zest']*n_houses*percent_hit\n",
    "    damage_mid_tier = houses['mid_zest']*n_houses*percent_hit\n",
    "    damage_top_tier = houses[\"top_zest\"]*n_houses*percent_hit\n",
    "    \n",
    "    \n",
    "#     print(place_value(damage_low_tier))\n",
    "#     print(place_value(damage_mid_tier))\n",
    "#     print(place_value(damage_top_tier))\n",
    "    return(damage_low_tier, damage_mid_tier, damage_top_tier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(zipcode):\n",
    "    \n",
    "    quandl = get_quandl(zipcode)\n",
    "    search = SearchEngine(simple_zipcode=False)\n",
    "    zdict = search.by_zipcode(zipcode).to_dict()\n",
    "    for key in quandl['ZHVIAH'].keys():\n",
    "        if key == 'quandl_error':\n",
    "            zest = zdict['median_home_value']\n",
    "            if type(zest) != float:\n",
    "                zest = 0\n",
    "            pp_sqft = 100\n",
    "            low_zest = zest * .33\n",
    "            mid_zest = zest *.51\n",
    "            top_zest = zest * 1.66\n",
    "            \n",
    "        else:\n",
    "            zest, low_zest, mid_zest, top_zest, pp_sqft = [quandl[key]['dataset']['data'][0][1] for key in quandl.keys()]\n",
    "\n",
    "        \n",
    "        \n",
    "    avg_sqft = zest / pp_sqft\n",
    "    hurr_1, hurr_2, hurr_3, hurr_4, hurr_5 = [hurricane_damage(zipcode, disaster_level=cat) for cat in range(1,6)]\n",
    "    flood_inches = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 24, 36, 48)\n",
    "    cats = range(0,6)\n",
    "    tor_0, tor_1, tor_2, tor_3, tor_4, tor_5 = [tornado_damage(zipcode, cat, low_zest, mid_zest, top_zest) for cat in cats]\n",
    "    \n",
    "    \n",
    "    \n",
    "    zip_dict = {'zip' : zipcode,\n",
    "                   'hv_median' : zest,\n",
    "                   'hv_lowtier' : low_zest,\n",
    "                   'hv_midtier' : mid_zest,\n",
    "                   'hv_toptier' : top_zest,\n",
    "                   'hurr_1' : hurr_1,\n",
    "                   'hurr_2' : hurr_2,\n",
    "                   'hurr_3' : hurr_3,\n",
    "                   'hurr_4' : hurr_4,\n",
    "                   'hurr_5' : hurr_5,\n",
    "                   'fl_1' : flood_loss(zipcode, 1, avg_sqft),\n",
    "                   'fl_2' : flood_loss(zipcode, 2, avg_sqft),\n",
    "                   'fl_3' : flood_loss(zipcode, 3, avg_sqft),\n",
    "                   'fl_4' : flood_loss(zipcode, 4, avg_sqft),\n",
    "                   'fl_5' : flood_loss(zipcode, 5, avg_sqft),\n",
    "                   'fl_6' : flood_loss(zipcode, 6, avg_sqft),\n",
    "                   'fl_7' : flood_loss(zipcode, 7, avg_sqft),\n",
    "                   'fl_8' : flood_loss(zipcode, 8, avg_sqft),\n",
    "                   'fl_9' : flood_loss(zipcode, 9, avg_sqft),\n",
    "                   'fl_10' : flood_loss(zipcode, 10, avg_sqft),\n",
    "                   'fl_11' : flood_loss(zipcode, 11, avg_sqft),\n",
    "                   'fl_12' : flood_loss(zipcode, 12, avg_sqft),\n",
    "                   'fl_24' : flood_loss(zipcode, 24, avg_sqft),\n",
    "                   'fl_36' : flood_loss(zipcode, 36, avg_sqft),\n",
    "                   'fl_48' :  flood_loss(zipcode, 48, avg_sqft),\n",
    "                   'tor_0_lt' : tor_0[0],\n",
    "                   'tor_1_lt' : tor_1[0],\n",
    "                   'tor_2_lt' : tor_2[0],\n",
    "                   'tor_3_lt' : tor_3[0],\n",
    "                   'tor_4_lt' : tor_4[0],\n",
    "                   'tor_5_lt' : tor_5[0],\n",
    "                   'tor_0_mt' : tor_0[1],\n",
    "                   'tor_1_mt' : tor_1[1],\n",
    "                   'tor_2_mt' : tor_2[1],\n",
    "                   'tor_3_mt' : tor_3[1],\n",
    "                   'tor_4_mt' : tor_4[1],\n",
    "                   'tor_5_mt' : tor_5[1],\n",
    "                   'tor_0_tt' : tor_0[2],\n",
    "                   'tor_1_tt' : tor_1[2],\n",
    "                   'tor_2_tt' : tor_2[2],\n",
    "                   'tor_3_tt' : tor_3[2],\n",
    "                   'tor_4_tt' : tor_4[2],\n",
    "                   'tor_5_tt' : tor_5[2],\n",
    "                'last_updated' : time.time()\n",
    "               }\n",
    "    print(f'Gathered stats for {zipcode}.')\n",
    "    return(zip_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_to_csv(zipcode):\n",
    "    try:\n",
    "        stat = os.stat(f'datasets/zips/data/{zipcode}.csv')\n",
    "        if time.time() - stat.st_mtime >= 6: #04800:\n",
    "            update = True\n",
    "    except:\n",
    "        update = True\n",
    "    \n",
    "    if update == True:\n",
    "            import csv\n",
    "            dict_for_df = get_dict(zipcode)\n",
    "            with open(f'datasets/zips/data/{zipcode}.csv', 'w') as f:\n",
    "                w = csv.DictWriter(f, dict_for_df.keys())\n",
    "                w.writeheader()\n",
    "                w.writerow(dict_for_df)\n",
    "                print(f'CSV data written for {zipcode}.')\n",
    "    else:\n",
    "        print(f\"CSV for {zipcode} up to date.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_to_sql(zipcode):\n",
    "    drop = False\n",
    "    update = True\n",
    "    exists = False\n",
    "    sql_complete = False\n",
    "    host = \"localhost\"\n",
    "    dbname = \"zip_codes\"\n",
    "    user = \"justinaugust\"\n",
    "    port =  5432\n",
    "    table_name = 'data'\n",
    "    \n",
    "    engine = sal.create_engine(f'postgresql://{user}@{host}:{port}/{dbname}')\n",
    "\n",
    "\n",
    "    with engine.connect() as conn, conn.begin():\n",
    "        check_exist = f\"\"\"select last_updated from data where zip = {zipcode};\"\"\"\n",
    "        last_updated = conn.execute(check_exist)\n",
    "        time_diff = -1\n",
    "        for row in last_updated:\n",
    "            last_updated = row[0]\n",
    "        if (type(last_updated) == float):\n",
    "            time_diff = last_updated - time.time()\n",
    "\n",
    "        \n",
    "            if (time_diff > 604800):\n",
    "                del_cmd = sal.sql.text(f\"\"\"DELETE FROM data WHERE \"zip\" = {zipcode}\"\"\")\n",
    "                del_ = conn.execute(del_cmd)\n",
    "                if conn.execute(\"commit\"):\n",
    "                    print(f'Deleted outdated data')\n",
    "            else:\n",
    "                update = False\n",
    "\n",
    "        if update == True:\n",
    "            dict_for_sql = get_dict(zipcode)\n",
    "            keys = '\"' + '\", \"'.join([key for key in dict_for_sql.keys()]) + '\"'\n",
    "            vals = ', '.join([str(val) for val in dict_for_sql.values()])\n",
    "            insert_command = sal.sql.text(f\"\"\"INSERT INTO data({keys}) VALUES({vals});\"\"\")\n",
    "            commit = conn.execute(insert_command)\n",
    "            if conn.execute(\"commit\"):\n",
    "                print(f'SQL updated for {zipcode}.')\n",
    "        else:\n",
    "            print(f'SQL doesn\\'t need to be updated for {zipcode}.')\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## based on code from https://stackoverflow.com/a/3964691\n",
    "\n",
    "def get_fetched():    \n",
    "    return([file.strip('.csv') for file in os.listdir(\"datasets/zips/data\")])\n",
    "\n",
    "def check_zips(zip_codes):\n",
    "    fetched = get_fetched()\n",
    "    \n",
    "    in_db = [zipcode for zipcode in zip_codes if zipcode in fetched]\n",
    "    not_in_db = [zipcode for zipcode in zip_codes if zipcode not in fetched]\n",
    "    \n",
    "    return(not_in_db, in_db)\n",
    "\n",
    "def have_geojson():\n",
    "    return([file.strip('.geojson') for file in os.listdir(\"datasets/zips/shapefiles\") if file.endswith('.geojson')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_zip(zipcode):\n",
    "    zip_to_csv(zipcode)\n",
    "#     zip_to_sql(zipcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_zips(zip_codes):\n",
    "    if (type(zip_codes) == str):\n",
    "            zip_codes = [zip_code.strip() for zip_code in zip_codes.split(\",\")]\n",
    "    zip_codes = [str(zipcode) for zipcode in zip_codes]\n",
    "    zip_codes = list(set(zip_codes))\n",
    "    \n",
    "    search = SearchEngine(simple_zipcode=False)\n",
    "    \n",
    "    have_shapes = have_geojson()\n",
    "    zip_codes = [zipcode for zipcode in zip_codes if zipcode in have_shapes]\n",
    "    zip_codes = [zipcode for zipcode in zip_codes if search.by_zipcode(zipcode).to_dict()['zipcode_type'] != 'PO Box']\n",
    "    not_fetched, fetched = check_zips(zip_codes)\n",
    "    \n",
    "    \n",
    "\n",
    "    for zipcode in not_fetched:\n",
    "        fetched = []\n",
    "        attempted = []\n",
    "        try:\n",
    "            update_zip(zipcode)\n",
    "            print(f'Fetched {zipcode}')\n",
    "            fetched.append(zipcode) \n",
    "            attempted.append(zipcode)\n",
    "        except:\n",
    "            print(f'could not fetch for {zipcode}')\n",
    "            attempted.append(zipcode)\n",
    "          \n",
    "        if (len(attempted) % 10000 == 0):\n",
    "            print('You\\'ll have to wait one day to get more information.')\n",
    "#             for i in range(86400):\n",
    "#                 sleep(1)\n",
    "#                 if i % 1000 == 0:\n",
    "#                     print(f'only {(86400 - i)/60} more minutes to wait... ', end='')\n",
    "    \n",
    "    print(' Done!')\n",
    "    not_fetched, zip_codes = check_zips(zip_codes)\n",
    "    return(zip_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"localhost\"\n",
    "dbname = \"zip_codes\"\n",
    "user = \"justinaugust\"\n",
    "port =  5432\n",
    "table_name = 'shapes'\n",
    "have_shapes = have_geojson()\n",
    "\n",
    "engine = sal.create_engine(f'postgresql://{user}@{host}:{port}/{dbname}')\n",
    "# filling the empty table with data!\n",
    "def shapes_to_sql():\n",
    "    for zipc in have_shapes:\n",
    "    #     try:\n",
    "        df = gpd.read_file(f\"datasets/zips/shapefiles/{zipc}.geojson\")\n",
    "        df['geometry'] = df['geometry'].map(wkb_hexer)\n",
    "        with engine.connect() as conn, conn.begin():\n",
    "            df.to_sql(table_name,\n",
    "                     con = conn,\n",
    "                     if_exists = 'append',\n",
    "                     index = False)\n",
    "    #     except:\n",
    "    #         print(f'{zipc} not in SQL')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dsi] *",
   "language": "python",
   "name": "conda-env-.conda-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
